{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T19:07:05.914178Z",
     "start_time": "2024-12-12T19:07:05.900455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n"
   ],
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T19:07:05.959917Z",
     "start_time": "2024-12-12T19:07:05.941467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class UCF101Dataset(Dataset):\n",
    "    \"\"\"UCF101 Dataset for video classification.\"\"\"\n",
    "\n",
    "    def __init__(self, video_dir, label_file, label_map, transform=None, keyframe_interval=10, num_frames=10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            video_dir (string): Path to the directory with video frames or video clips.\n",
    "            label_file (string): Path to the label file (e.g., train_labels.txt).\n",
    "            label_map (dict): Mapping of class names to label indices.\n",
    "            transform (callable, optional): Transform to be applied to each frame/clip.\n",
    "            keyframe_interval (int, optional): Extract one frame every `keyframe_interval` frames.\n",
    "            num_frames (int, optional): Fixed number of frames per video.\n",
    "        \"\"\"\n",
    "        self.video_dir = video_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.keyframe_interval = keyframe_interval\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "        # Load video paths and labels from the label file\n",
    "        with open(label_file, 'r') as file:\n",
    "            for line in file:\n",
    "                video_name, class_name = line.strip().split()\n",
    "                video_path = os.path.join(video_dir, class_name, video_name)\n",
    "                label = label_map[class_name]\n",
    "                self.samples.append((video_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path, label = self.samples[idx]\n",
    "\n",
    "        # Load video frames\n",
    "        frames = self.load_video_frames(video_path)\n",
    "\n",
    "        # Truncate or pad frames to ensure a fixed number of frames\n",
    "        if len(frames) > self.num_frames:\n",
    "            frames = frames[:self.num_frames]\n",
    "        elif len(frames) < self.num_frames:\n",
    "            pad_size = self.num_frames - len(frames)\n",
    "            padding = [torch.zeros_like(frames[0])] * pad_size\n",
    "            frames.extend(padding)\n",
    "\n",
    "        frames = torch.stack(frames)  # Combine frames into a single tensor\n",
    "\n",
    "        return {'video': frames, 'label': label, 'path': video_path}\n",
    "\n",
    "    def load_video_frames(self, video_path):\n",
    "        \"\"\"Load video frames at regular intervals.\"\"\"\n",
    "        import cv2\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        frame_idx = 0\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if frame_idx % self.keyframe_interval == 0:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = Image.fromarray(frame)\n",
    "                if self.transform:\n",
    "                    frame = self.transform(frame)\n",
    "                frames.append(frame)\n",
    "            frame_idx += 1\n",
    "        cap.release()\n",
    "        return frames\n"
   ],
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T19:07:05.994724Z",
     "start_time": "2024-12-12T19:07:05.982797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle varying frame counts in a batch.\n",
    "    \"\"\"\n",
    "    videos = [item['video'] for item in batch]\n",
    "    labels = [item['label'] for item in batch]\n",
    "    paths = [item['path'] for item in batch]\n",
    "\n",
    "    # Find the maximum number of frames in the batch\n",
    "    max_frames = max(video.size(0) for video in videos)\n",
    "\n",
    "    # Pad all videos to the maximum number of frames\n",
    "    padded_videos = []\n",
    "    for video in videos:\n",
    "        pad_size = max_frames - video.size(0)\n",
    "        if pad_size > 0:\n",
    "            padding = torch.zeros((pad_size, *video.size()[1:]))\n",
    "            video = torch.cat([video, padding], dim=0)\n",
    "        padded_videos.append(video)\n",
    "\n",
    "    padded_videos = torch.stack(padded_videos)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    return {'video': padded_videos, 'label': labels, 'path': paths}\n"
   ],
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T19:07:06.095792Z",
     "start_time": "2024-12-12T19:07:06.014680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate the label map\n",
    "video_dir = r'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/'\n",
    "class_names = sorted(os.listdir(video_dir))  # Folder names in the train directory\n",
    "label_map = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "\n",
    "# Paths to label files\n",
    "train_label_file = r'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/train_labels.txt'\n",
    "val_label_file = r'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/val_labels.txt'\n",
    "test_label_file = r'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/test_labels.txt'\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = UCF101Dataset(\n",
    "    video_dir=r'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/', \n",
    "    label_file=train_label_file, \n",
    "    label_map=label_map,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    keyframe_interval=5  # Extract 1 frame every 10 frames\n",
    ")\n",
    "\n",
    "val_dataset = UCF101Dataset(\n",
    "    video_dir=r'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/', \n",
    "    label_file=val_label_file, \n",
    "    label_map=label_map,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    keyframe_interval=5\n",
    ")\n",
    "\n",
    "test_dataset = UCF101Dataset(\n",
    "    video_dir=r'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/', \n",
    "    label_file=test_label_file, \n",
    "    label_map=label_map,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    keyframe_interval=5\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0, collate_fn=custom_collate_fn)"
   ],
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T19:07:22.585204Z",
     "start_time": "2024-12-12T19:07:06.121837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def verify_dataloader(dataloader, dataset_name):\n",
    "    \"\"\"\n",
    "    Verify the correctness of the DataLoader by iterating over a few batches.\n",
    "\n",
    "    Args:\n",
    "        dataloader: DataLoader to verify.\n",
    "        dataset_name: Name of the dataset being verified (train, val, test).\n",
    "    \"\"\"\n",
    "    print(f\"Verifying {dataset_name} DataLoader...\")\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        print(f\"Batch {i + 1}:\")\n",
    "        print(f\"Video Tensor Shape: {batch['video'].shape}\")  # Should be (batch_size, num_keyframes, channels, height, width)\n",
    "        print(f\"Labels: {batch['label']}\")  # Ensure labels are mapped correctly\n",
    "        print(f\"Paths: {batch['path']}\")  # Check if file paths are correct\n",
    "\n",
    "        # Stop after a few batches\n",
    "        if i >= 2:\n",
    "            break\n",
    "\n",
    "    print(f\"{dataset_name} DataLoader verified!\\n\")\n",
    "\n",
    "# Verify train, val, and test loaders\n",
    "verify_dataloader(train_loader, \"Train\")\n",
    "verify_dataloader(val_loader, \"Validation\")\n",
    "verify_dataloader(test_loader, \"Test\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying Train DataLoader...\n",
      "Batch 1:\n",
      "Video Tensor Shape: torch.Size([16, 10, 3, 128, 128])\n",
      "Labels: tensor([27, 73, 41, 65, 89, 55, 33,  6, 15, 30, 62, 21,  6, 14, 26, 61])\n",
      "Paths: ['C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/Fencing\\\\v_Fencing_g10_c02.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/RockClimbingIndoor\\\\v_RockClimbingIndoor_g23_c02.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/HorseRiding\\\\v_HorseRiding_g20_c03.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/PlayingTabla\\\\v_PlayingTabla_g22_c04.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/TableTennisShot\\\\v_TableTennisShot_g24_c02.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/Nunchucks\\\\v_Nunchucks_g13_c06.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/Haircut\\\\v_Haircut_g09_c05.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/BaseballPitch\\\\v_BaseballPitch_g07_c05.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/Bowling\\\\v_Bowling_g06_c02.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/FrisbeeCatch\\\\v_FrisbeeCatch_g02_c02.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/PlayingGuitar\\\\v_PlayingGuitar_g12_c03.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/CliffDiving\\\\v_CliffDiving_g18_c01.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/BaseballPitch\\\\v_BaseballPitch_g25_c07.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/BodyWeightSquats\\\\v_BodyWeightSquats_g20_c01.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/Drumming\\\\v_Drumming_g03_c04.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/PlayingFlute\\\\v_PlayingFlute_g04_c03.avi']\n",
      "Batch 2:\n",
      "Video Tensor Shape: torch.Size([16, 10, 3, 128, 128])\n",
      "Labels: tensor([ 24,   6,  10,   4,  93,  71,  61,  29,  52,   2,   0,  64,  69,  75,\n",
      "         84, 100])\n",
      "Paths: ['C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/CuttingInKitchen\\\\v_CuttingInKitchen_g07_c01.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/BaseballPitch\\\\v_BaseballPitch_g22_c07.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/Biking\\\\v_Biking_g10_c02.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/BalanceBeam\\\\v_BalanceBeam_g17_c05.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/TrampolineJumping\\\\v_TrampolineJumping_g16_c05.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/PushUps\\\\v_PushUps_g13_c02.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/PlayingFlute\\\\v_PlayingFlute_g12_c04.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/FloorGymnastics\\\\v_FloorGymnastics_g20_c01.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/MilitaryParade\\\\v_MilitaryParade_g14_c04.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/Archery\\\\v_Archery_g13_c07.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g08_c04.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/PlayingSitar\\\\v_PlayingSitar_g21_c06.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/PullUps\\\\v_PullUps_g22_c01.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/Rowing\\\\v_Rowing_g14_c07.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/SoccerPenalty\\\\v_SoccerPenalty_g19_c01.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/YoYo\\\\v_YoYo_g20_c04.avi']\n",
      "Batch 3:\n",
      "Video Tensor Shape: torch.Size([16, 10, 3, 128, 128])\n",
      "Labels: tensor([28, 62, 84,  9, 78, 53, 88, 65, 55, 55, 74, 28,  9, 36,  8,  4])\n",
      "Paths: ['C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/FieldHockeyPenalty\\\\v_FieldHockeyPenalty_g14_c06.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/PlayingGuitar\\\\v_PlayingGuitar_g19_c02.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/SoccerPenalty\\\\v_SoccerPenalty_g10_c02.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/BenchPress\\\\v_BenchPress_g07_c07.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/Shotput\\\\v_Shotput_g15_c04.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/Mixing\\\\v_Mixing_g11_c05.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/Swing\\\\v_Swing_g12_c04.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/PlayingTabla\\\\v_PlayingTabla_g05_c03.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/Nunchucks\\\\v_Nunchucks_g12_c05.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/Nunchucks\\\\v_Nunchucks_g05_c03.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/RopeClimbing\\\\v_RopeClimbing_g02_c03.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/FieldHockeyPenalty\\\\v_FieldHockeyPenalty_g24_c01.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/BenchPress\\\\v_BenchPress_g13_c03.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/HandstandPushups\\\\v_HandStandPushups_g11_c04.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/BasketballDunk\\\\v_BasketballDunk_g22_c03.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/BalanceBeam\\\\v_BalanceBeam_g14_c02.avi']\n",
      "Train DataLoader verified!\n",
      "\n",
      "Verifying Validation DataLoader...\n",
      "Batch 1:\n",
      "Video Tensor Shape: torch.Size([16, 10, 3, 128, 128])\n",
      "Labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Paths: ['C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g03_c02.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g25_c05.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g19_c04.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g22_c05.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g08_c01.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g18_c04.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g10_c05.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g14_c01.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g02_c02.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g04_c02.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g23_c04.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g07_c01.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g09_c02.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g14_c02.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g22_c03.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g15_c03.avi']\n",
      "Batch 2:\n",
      "Video Tensor Shape: torch.Size([16, 10, 3, 128, 128])\n",
      "Labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1])\n",
      "Paths: ['C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g10_c04.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g17_c01.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g16_c03.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g24_c05.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g16_c01.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g01_c06.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g15_c01.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g08_c02.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g25_c03.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g05_c06.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g12_c03.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g13_c06.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g13_c02.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyLipstick\\\\v_ApplyLipstick_g03_c04.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyLipstick\\\\v_ApplyLipstick_g07_c01.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyLipstick\\\\v_ApplyLipstick_g12_c02.avi']\n",
      "Batch 3:\n",
      "Video Tensor Shape: torch.Size([16, 10, 3, 128, 128])\n",
      "Labels: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Paths: ['C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyLipstick\\\\v_ApplyLipstick_g22_c06.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyLipstick\\\\v_ApplyLipstick_g01_c04.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyLipstick\\\\v_ApplyLipstick_g05_c04.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyLipstick\\\\v_ApplyLipstick_g12_c03.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyLipstick\\\\v_ApplyLipstick_g22_c02.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyLipstick\\\\v_ApplyLipstick_g17_c04.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyLipstick\\\\v_ApplyLipstick_g06_c01.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyLipstick\\\\v_ApplyLipstick_g05_c01.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyLipstick\\\\v_ApplyLipstick_g18_c02.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyLipstick\\\\v_ApplyLipstick_g08_c04.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyLipstick\\\\v_ApplyLipstick_g22_c01.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyLipstick\\\\v_ApplyLipstick_g23_c02.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyLipstick\\\\v_ApplyLipstick_g09_c04.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyLipstick\\\\v_ApplyLipstick_g11_c02.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyLipstick\\\\v_ApplyLipstick_g23_c04.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/val/ApplyLipstick\\\\v_ApplyLipstick_g17_c01.avi']\n",
      "Validation DataLoader verified!\n",
      "\n",
      "Verifying Test DataLoader...\n",
      "Batch 1:\n",
      "Video Tensor Shape: torch.Size([16, 10, 3, 128, 128])\n",
      "Labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Paths: ['C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g07_c02.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g23_c01.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g23_c03.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g15_c04.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g21_c05.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g14_c05.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g25_c02.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g02_c04.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g13_c03.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g03_c03.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g17_c03.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g11_c02.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g22_c04.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g24_c04.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g12_c01.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g05_c01.avi']\n",
      "Batch 2:\n",
      "Video Tensor Shape: torch.Size([16, 10, 3, 128, 128])\n",
      "Labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1])\n",
      "Paths: ['C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g11_c01.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g22_c01.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g15_c07.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g12_c05.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g12_c02.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g21_c02.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g11_c05.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g23_c02.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g15_c06.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g25_c06.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g16_c02.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g07_c05.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyEyeMakeup\\\\v_ApplyEyeMakeup_g07_c03.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyLipstick\\\\v_ApplyLipstick_g03_c01.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyLipstick\\\\v_ApplyLipstick_g15_c05.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyLipstick\\\\v_ApplyLipstick_g15_c03.avi']\n",
      "Batch 3:\n",
      "Video Tensor Shape: torch.Size([16, 10, 3, 128, 128])\n",
      "Labels: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Paths: ['C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyLipstick\\\\v_ApplyLipstick_g15_c04.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyLipstick\\\\v_ApplyLipstick_g18_c04.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyLipstick\\\\v_ApplyLipstick_g20_c05.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyLipstick\\\\v_ApplyLipstick_g20_c02.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyLipstick\\\\v_ApplyLipstick_g15_c02.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyLipstick\\\\v_ApplyLipstick_g17_c05.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyLipstick\\\\v_ApplyLipstick_g01_c05.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyLipstick\\\\v_ApplyLipstick_g05_c05.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyLipstick\\\\v_ApplyLipstick_g02_c01.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyLipstick\\\\v_ApplyLipstick_g25_c04.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyLipstick\\\\v_ApplyLipstick_g13_c04.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyLipstick\\\\v_ApplyLipstick_g08_c01.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyLipstick\\\\v_ApplyLipstick_g16_c02.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyLipstick\\\\v_ApplyLipstick_g05_c02.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyLipstick\\\\v_ApplyLipstick_g07_c04.avi', 'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/test/ApplyLipstick\\\\v_ApplyLipstick_g23_c03.avi']\n",
      "Test DataLoader verified!\n",
      "\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T19:07:22.635427Z",
     "start_time": "2024-12-12T19:07:22.622146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import recall_score, accuracy_score"
   ],
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T19:07:24.259952Z",
     "start_time": "2024-12-12T19:07:22.657555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dtype = torch.FloatTensor  # Set dtype to FloatTensor for CPU\n",
    "\n",
    "# Define Flatten layer\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.size()  # Read in N, C, H, W\n",
    "        return x.view(N, -1)  # Flatten the C * H * W values into a single vector per image\n",
    "    \n",
    "    \n",
    "class Flatten3d(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)  # Flatten all dimensions except the batch size\n",
    "\n",
    "# Define fixed model\n",
    "num_classes = len(label_map)\n",
    "# fixed_model_base = nn.Sequential(\n",
    "#     nn.Conv2d(3, 8, kernel_size=7, stride=1),\n",
    "#     nn.ReLU(inplace=True),\n",
    "#     nn.MaxPool2d(2, stride=2),\n",
    "#     nn.Conv2d(8, 16, kernel_size=7, stride=1),\n",
    "#     nn.ReLU(inplace=True),\n",
    "#     nn.MaxPool2d(2, stride=2),\n",
    "#     Flatten(),\n",
    "#     nn.ReLU(inplace=True),\n",
    "#     nn.Linear(1936, num_classes)\n",
    "# )\n",
    "\n",
    "fixed_model_base = nn.Sequential(\n",
    "    nn.Conv3d(in_channels=3, out_channels=64, kernel_size=2, stride=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool3d((1, 2, 2), stride=(1, 2, 2)),\n",
    "    \n",
    "    nn.Conv3d(in_channels=64, out_channels=256, kernel_size=(1, 3, 3), stride=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool3d((1, 3, 3), stride=(1, 2, 2)),\n",
    "\n",
    "    nn.Dropout3d(0.1),\n",
    "    Flatten3d(),\n",
    "\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(2073600, num_classes),  # Adjust dimensions based on the output of conv layers\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "\n",
    "\n",
    "fixed_model = fixed_model_base.type(dtype)  # Set model to the appropriate data type\n",
    "\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(fixed_model.parameters(), lr=1e-4)  # Learning rate set to 1e-3\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T19:07:24.323805Z",
     "start_time": "2024-12-12T19:07:24.306109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def train(model, loss_fn, optimizer, train_loader, val_loader, num_epochs=10, print_every=100):\n",
    "#     model.train()  # Set the model to training mode\n",
    "#     for epoch in range(num_epochs):\n",
    "#         print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "#         running_loss = 0.0\n",
    "#         for i, batch in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
    "#             # Debugging: Check the structure of the batch\n",
    "#             assert 'video' in batch and 'label' in batch, \"Batch does not have 'video' or 'label' keys\"\n",
    "#             assert batch['video'].dim() == 5, \"Video tensor should have 5 dimensions [batch_size, num_frames, 3, H, W]\"\n",
    "#             assert batch['label'].dim() == 1, \"Label tensor should have 1 dimension [batch_size]\"\n",
    "# \n",
    "#             # Flatten the time dimension into the batch size\n",
    "#             videos = batch['video']  # Shape: [batch_size, num_frames, 3, H, W]\n",
    "#             batch_size, num_frames, C, H, W = videos.size()\n",
    "#             x_var = videos.view(batch_size * num_frames, C, H, W).to(torch.float)  # Shape: [batch_size * num_frames, 3, H, W]\n",
    "#             y_var = batch['label'].repeat_interleave(num_frames).to(torch.long)  # Repeat labels for all frames\n",
    "# \n",
    "#             # Forward pass\n",
    "#             scores = model(x_var)\n",
    "#             loss = loss_fn(scores, y_var)\n",
    "# \n",
    "#             # Backward pass\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "# \n",
    "#             # Update weights\n",
    "#             optimizer.step()\n",
    "# \n",
    "#             running_loss += loss.item()\n",
    "# \n",
    "#             if (i + 1) % print_every == 0:\n",
    "#                 print(f\"Batch {i + 1}, Loss: {loss.item():.4f}\")\n",
    "# \n",
    "#         # Validate the model\n",
    "#         validate(model, val_loader)\n",
    "# \n",
    "# # Updated validate function\n",
    "# # Updated validate function\n",
    "# def validate(model, val_loader):\n",
    "#     model.eval()  # Set the model to evaluation mode\n",
    "#     all_preds = []\n",
    "#     all_labels = []\n",
    "#     with torch.no_grad():\n",
    "#         for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "#             assert 'video' in batch and 'label' in batch, \"Batch does not have 'video' or 'label' keys\"\n",
    "#             videos = batch['video']  # Shape: [batch_size, num_frames, 3, H, W]\n",
    "#             batch_size, num_frames, C, H, W = videos.size()\n",
    "# \n",
    "#             # Correct usage of `.to()` with both device and dtype\n",
    "#             x_var = videos.view(batch_size * num_frames, C, H, W).to(torch.device('cpu'), torch.float)\n",
    "#             y_var = batch['label'].repeat_interleave(num_frames).long().to(torch.device('cpu'))\n",
    "# \n",
    "#             scores = model(x_var)\n",
    "#             _, preds = torch.max(scores, 1)\n",
    "#             all_preds.extend(preds.cpu().numpy())\n",
    "#             all_labels.extend(y_var.cpu().numpy())\n",
    "# \n",
    "#     acc = accuracy_score(all_labels, all_preds)\n",
    "#     recall = recall_score(all_labels, all_preds, average='macro')\n",
    "#     print(f\"Validation Accuracy: {acc * 100:.2f}%\")\n",
    "#     print(f\"Validation Recall: {recall * 100:.2f}%\")\n",
    "# \n",
    "# # def test(model, test_loader):\n",
    "# #     model.eval()  # Set the model to evaluation mode\n",
    "# #     all_preds = []\n",
    "# #     all_labels = []\n",
    "# #     with torch.no_grad():\n",
    "# #         for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "# #             x_var = Variable(batch['video'])\n",
    "# #             y_var = Variable(batch['label']).long()\n",
    "# #             scores = model(x_var)\n",
    "# #             _, preds = torch.max(scores, 1)\n",
    "# #             all_preds.extend(preds.cpu().numpy())\n",
    "# #             all_labels.extend(y_var.cpu().numpy())\n",
    "# #     acc = accuracy_score(all_labels, all_preds)\n",
    "# #     recall = recall_score(all_labels, all_preds, average='macro')\n",
    "# #     print(f\"Test Accuracy: {acc * 100:.2f}%\")\n",
    "# #     print(f\"Test Recall: {recall * 100:.2f}%\")"
   ],
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T19:07:24.394519Z",
     "start_time": "2024-12-12T19:07:24.365311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(model, loss_fn, optimizer, train_loader, val_loader, num_epochs=10, print_every=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        running_loss = 0.0\n",
    "        for i, batch in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
    "            # Prepare input\n",
    "            videos = batch['video']  # Shape: [batch_size, num_frames, 3, H, W]\n",
    "            videos = videos.permute(0, 2, 1, 3, 4).to(torch.float32)  # Shape: [batch_size, 3, num_frames, H, W]\n",
    "            labels = batch['label'].to(torch.long)\n",
    "\n",
    "            # Forward pass\n",
    "            scores = model(videos)\n",
    "            loss = loss_fn(scores, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if (i + 1) % print_every == 0:\n",
    "                print(f\"Batch {i + 1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Validate the model\n",
    "        #validate(model, val_loader)\n",
    "\n",
    "def validate(model, val_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            videos = batch['video']\n",
    "            videos = videos.permute(0, 2, 1, 3, 4).to(torch.float32)\n",
    "            labels = batch['label'].to(torch.long)\n",
    "\n",
    "            scores = model(videos)\n",
    "            _, preds = torch.max(scores, 1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    print(f\"Validation Accuracy: {acc * 100:.2f}%\")\n",
    "    print(f\"Validation Recall: {recall * 100:.2f}%\")"
   ],
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-12T19:07:24.441618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train, validate, and test the model\n",
    "train(fixed_model, loss_fn, optimizer, train_loader, val_loader, num_epochs=5, print_every=100)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 100/498 [12:41<48:26,  7.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100, Loss: 3.9432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 200/498 [22:42<28:32,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200, Loss: 2.3896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 300/498 [34:08<23:16,  7.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300, Loss: 2.3579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 400/498 [44:55<10:51,  6.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400, Loss: 0.9705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 498/498 [54:42<00:00,  6.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 100/498 [11:05<44:21,  6.69s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100, Loss: 0.1438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 200/498 [22:34<34:57,  7.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200, Loss: 0.0703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 300/498 [34:24<22:46,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300, Loss: 0.0630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 400/498 [46:11<12:11,  7.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400, Loss: 0.0055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 498/498 [57:39<00:00,  6.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 100/498 [11:53<46:18,  6.98s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100, Loss: 0.0191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 200/498 [23:40<36:33,  7.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200, Loss: 0.0054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 300/498 [35:48<22:41,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300, Loss: 0.0087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 400/498 [47:55<13:10,  8.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400, Loss: 0.0059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 498/498 [59:19<00:00,  7.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 100/498 [11:13<42:46,  6.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100, Loss: 0.0015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 200/498 [22:56<30:55,  6.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200, Loss: 0.0037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 300/498 [33:22<23:34,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300, Loss: 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 400/498 [45:15<11:29,  7.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400, Loss: 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 498/498 [56:54<00:00,  6.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 100/498 [11:04<48:16,  7.28s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100, Loss: 0.0104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 200/498 [23:48<38:48,  7.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200, Loss: 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|████▏     | 206/498 [48:20<9:01:00, 111.16s/it] "
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T23:50:46.095891900Z",
     "start_time": "2024-12-12T17:39:56.810718Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T00:04:00.681802Z",
     "start_time": "2024-12-12T23:51:05.366768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Test\"):\n",
    "            videos = batch['video']\n",
    "            videos = videos.permute(0, 2, 1, 3, 4).to(torch.float32)\n",
    "            labels = batch['label'].to(torch.long)\n",
    "\n",
    "            scores = model(videos)\n",
    "            _, preds = torch.max(scores, 1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    print(f\"Validation Accuracy: {acc * 100:.2f}%\")\n",
    "    print(f\"Validation Recall: {recall * 100:.2f}%\")\n",
    "\n",
    "\n",
    "test(fixed_model, test_loader)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 171/171 [12:54<00:00,  4.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 71.24%\n",
      "Validation Recall: 70.30%\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T00:04:09.235305Z",
     "start_time": "2024-12-13T00:04:09.201630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import deque\n",
    "import torch"
   ],
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T00:04:10.829447Z",
     "start_time": "2024-12-13T00:04:10.800645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_label_map(label_file):\n",
    "    \"\"\"\n",
    "    Load the label mapping from the label file.\n",
    "    Args:\n",
    "        label_file: Path to the label file.\n",
    "    Returns:\n",
    "        label_map: Dictionary mapping indices to action labels.\n",
    "        reverse_map: Dictionary mapping action names to indices.\n",
    "    \"\"\"\n",
    "    label_map = {}\n",
    "    reverse_map = {}\n",
    "    with open(label_file, 'r') as f:\n",
    "        for line in f:\n",
    "            video_file, action_label = line.strip().split()\n",
    "            if action_label not in reverse_map:\n",
    "                idx = len(reverse_map)  # Assign a new index to unseen labels\n",
    "                reverse_map[action_label] = idx\n",
    "                label_map[idx] = action_label\n",
    "    return label_map, reverse_map\n"
   ],
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T00:04:12.937387Z",
     "start_time": "2024-12-13T00:04:12.903924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "label_file = r'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/test_labels.txt'\n",
    "label_map, reverse_map = load_label_map(label_file)"
   ],
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T00:04:14.620866Z",
     "start_time": "2024-12-13T00:04:14.539556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def detect_action_realtime(\n",
    "    model, label_map, device, video_source, output_file=\"output_video.mp4\", frame_interval=10, frame_size=(64, 64)\n",
    "):\n",
    "    \"\"\"\n",
    "    Real-time action detection using a trained model with video output.\n",
    "\n",
    "    Args:\n",
    "    - model: The trained PyTorch model.\n",
    "    - label_map: Dictionary mapping indices to action names.\n",
    "    - device: The device ('cpu' or 'cuda') where the model will run.\n",
    "    - video_source: Path to the video file or webcam index.\n",
    "    - output_file: Path to save the output video with action labels.\n",
    "    - frame_interval: Number of frames per segment to classify (e.g., 16).\n",
    "    - frame_size: Tuple indicating the size to which video frames are resized.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_source)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video source.\")\n",
    "        return\n",
    "\n",
    "    # Get video properties\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    # Set up video writer for saving output\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_file, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    # Frame buffer and prediction history\n",
    "    frame_buffer = deque(maxlen=frame_interval)\n",
    "    prediction_history = deque(maxlen=5)  # Store the last 5 predictions\n",
    "\n",
    "    # Transform to resize, convert to tensor, and normalize\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(frame_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Use tqdm to track the processing progress\n",
    "    with tqdm(total=frame_count, desc=\"Processing video\") as pbar:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            # Check if the frame is valid\n",
    "            if not ret or frame is None:\n",
    "                print(\"Error: Frame is None. Skipping this frame.\")\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                # Transform the frame\n",
    "                resized_frame = transform(frame)\n",
    "                frame_buffer.append(resized_frame)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in frame transformation: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Process frames if buffer is full\n",
    "            if len(frame_buffer) == frame_interval:\n",
    "                # Stack frames and adjust shape\n",
    "                input_frames = torch.stack(list(frame_buffer), dim=0)  # Shape: (frame_interval, C, H, W)\n",
    "                input_frames = input_frames.permute(1, 0, 2, 3).unsqueeze(0).to(device)  # Shape: (1, C, T, H, W)\n",
    "\n",
    "                # Model prediction\n",
    "                with torch.no_grad():\n",
    "                    scores = model(input_frames)\n",
    "                    _, preds = torch.max(scores, 1)\n",
    "\n",
    "                    # Handle KeyError gracefully if predicted label is not in label_map\n",
    "                    predicted_label = label_map.get(preds.item(), \"Unknown Action\")\n",
    "\n",
    "                # Smooth predictions using history\n",
    "                prediction_history.append(predicted_label)\n",
    "                smoothed_prediction = max(set(prediction_history), key=prediction_history.count)\n",
    "\n",
    "                # Display the smoothed prediction on the frame\n",
    "                cv2.putText(frame, f\"Action: {smoothed_prediction}\", (10, 50),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "            # Write the frame to the output video\n",
    "            out.write(frame)\n",
    "            pbar.update(1)\n",
    "\n",
    "            # Show the video in real-time\n",
    "            cv2.imshow(\"Real-Time Action Detection\", frame)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()"
   ],
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T00:46:13.791101Z",
     "start_time": "2024-12-13T00:46:02.706116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define label map\n",
    "video_dir = r'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new/dataset/train/'\n",
    "class_names = sorted(os.listdir(video_dir))  # Folder names in the train directory\n",
    "label_map = {idx: class_name for idx, class_name in enumerate(class_names)}\n",
    "\n",
    "# Paths for input and output videos\n",
    "video_file = r'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new3/dataset/test4.avi'\n",
    "output_file = r'C:/Users/22322/Desktop/UIUC/ECE549/final/new/new3/dataset/output_video4.mp4'\n",
    "\n",
    "# Run detection\n",
    "detect_action_realtime(\n",
    "    model=fixed_model,\n",
    "    label_map=label_map,\n",
    "    device=device,\n",
    "    video_source=video_file,\n",
    "    output_file=output_file,\n",
    "    frame_interval=10,\n",
    "    frame_size=(128, 128)\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video: 100%|██████████| 116/116 [00:10<00:00, 10.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Frame is None. Skipping this frame.\n"
     ]
    }
   ],
   "execution_count": 79
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
